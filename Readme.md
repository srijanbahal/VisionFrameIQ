# VisionFrameIQ: Multimodal Video Understanding & Retrieval

**"Bridging text, vision, and segmentation for intelligent video insights."**

## ğŸš€ Overview

VisionFrameIQ is a multimodal video intelligence system designed to:

- Extract video frames efficiently and embed them using CLIP for semantic search.
- Retrieve the most relevant frames for text queries (e.g., "person smiling while driving").
- Apply region highlighting using Meta's Segment Anything 2.1 (SAM 2.1) to localize query-relevant areas.
- Optionally refine masks with CLIPSeg for context-aware segmentation.
- Provide an interactive Streamlit-based interface for search, visualization, and progressive feedback.

## âœ¨ Key Features

| Capability | Description |
|------------|-------------|
| ğŸ¥ **Video Frame Extraction** | Extracts frames from uploaded videos at a configurable FPS. |
| ğŸ§© **Multimodal Embeddings (CLIP)** | Generates frame-level embeddings for semantic retrieval. |
| ğŸ” **Text-to-Video Search** | Finds frames most semantically similar to a natural language query. |
| ğŸ§  **Qdrant Vector Store** | Stores and retrieves embeddings efficiently with metadata. |
| ğŸª„ **Region Highlighting (SAM 2.1)** | Highlights the most relevant regions using Segment Anything 2.1. |
| ğŸ§¾ **CLIPSeg Refinement** | Optional fine-grained refinement of the SAM masks using CLIPSeg. |
| ğŸ–¥ï¸ **Interactive UI (Streamlit)** | Upload, process, and search across videos visually. |

## ğŸ—ï¸ Current Architecture

```
VisionFrameIQ/
â”‚
â”œâ”€â”€ app.py                  # Streamlit interface (upload, search, display)
â”œâ”€â”€ utils.py                # Frame extraction, embeddings, Qdrant helper functions
â”œâ”€â”€ segment.py              # CLIPSeg + SAM2.1-based region highlighting
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ qdrant_storage/         # Local persistent Qdrant store
â”œâ”€â”€ frames/                 # Extracted frames per video
â”‚   â”œâ”€â”€ test01/
â”‚   â””â”€â”€ test02/
â””â”€â”€ sample_videos/          # Uploaded video storage
```

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone & Setup Environment

```bash
git clone https://github.com/<your-username>/VisionFrameIQ.git
cd VisionFrameIQ

python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 2ï¸âƒ£ Install Dependencies

Ensure you have:

```bash
pip install torch torchvision --upgrade
pip install opencv-python pillow
pip install streamlit qdrant-client
pip install transformers supervision
pip install git+https://github.com/facebookresearch/segment-anything-2.git
```

### 3ï¸âƒ£ Run the App

```bash
streamlit run app.py
```

You can now:

- Upload videos (.mp4, .mov, etc.)
- Process & embed frames
- Enter text queries to find relevant video moments
- View highlighted regions (via SAM 2.1) directly in the Streamlit UI

## ğŸ§© How SAM 2.1 and CLIPSeg Work Here

### CLIP Embedding (retrieval)

- Each frame is encoded using a CLIP vision encoder; queries are encoded using the text encoder.
- Similarities are computed in the embedding space to find matching frames.

### SAM 2.1 (localization)

- Once frames are retrieved, SAM 2.1 segments the most relevant regions for visualization.

### CLIPSeg (refinement)

- CLIPSeg optionally refines masks using the text query, providing semantic alignment for finer visual correspondence.

## ğŸ“¸ Example Usage

```python
from segment import highlight_region_with_sam2

result = highlight_region_with_sam2("frames/test01/frame_0004.jpg")
result.show()
```

Or directly through the Streamlit app by uploading and querying.

## ğŸ§  Tech Stack

| Layer | Tools |
|-------|-------|
| **Retrieval Engine** | OpenAI CLIP / LLaVA embeddings, Qdrant |
| **Segmentation** | SAM 2.1 (facebook/sam2-hiera-base-plus), CLIPSeg |
| **Frontend** | Streamlit (currently), React (planned) |
| **Backend** | FastAPI / Python |
| **Storage** | Qdrant (vector DB), local persistent storage |
| **Deployment** | Streamlit Cloud / Render (planned) |

## ğŸ§ª Sample Queries

- "Person waving hand near car"
- "Dog jumping on sofa"
- "Logo appearing on the screen"
- "Close-up of smiling face"

## ğŸ§­ Performance & Notes

- Qdrant persistence ensures embeddings survive Streamlit reloads.
- SAM 2.1's BF16 inference provides faster GPU performance.
- Combining CLIPSeg + SAM improves localization accuracy.
- Works efficiently on a mid-tier GPU (e.g., RTX 3060 12 GB).

## ğŸ§± Future Roadmap & TODOs

### ğŸ§© Immediate TODOs

- [ ] Integrate SAM 2.1 region highlighting into top-K frame retrieval loop (automated pipeline).
- [ ] Implement CLIPSeg-guided mask refinement for better text alignment.
- [ ] Add query-refinement feedback loop: learn from user-marked relevant frames.
- [ ] Multi-video retrieval ranking: merge results from multiple videos.

### ğŸ§  Model & Retrieval Refinements

- [ ] Add hybrid embedding fusion (visual + textual weighting).
- [ ] Introduce temporal continuity for neighboring frames (context-aware retrieval).
- [ ] Implement subclip extraction â€” generate short GIFs / 5-sec clips from retrieved frames.
- [ ] Add region-based scoring visualization (heatmaps).

### âš™ï¸ Backend TODOs

- [ ] Build FastAPI routes:
  - `/upload_video`
  - `/process_video`
  - `/search`
  - `/feedback`
- [ ] Make modular service layer for embeddings, Qdrant, and SAM.
- [ ] Create a `config.py` to unify paths, model IDs, and environment variables.

### ğŸ’» Frontend TODOs

- [ ] React-based frontend using Next.js or Vite.
- [ ] Integrate video preview, frame timeline slider, and region overlays.
- [ ] Connect React UI to FastAPI backend (replacing Streamlit).
- [ ] Support drag-and-drop video uploads.
- [ ] Add search history & feedback dashboard.

### â˜ï¸ Deployment TODOs

- [ ] Containerize via Docker.
- [ ] Deploy backend (FastAPI + Qdrant) on Render or Railway.
- [ ] Host frontend on Vercel or Netlify.
- [ ] Optionally integrate with Hugging Face Spaces (Gradio) for public demo.

### ğŸ”¬ Stretch Goals

- [ ] Introduce video-LLM commentary (text summary of matched subclips).
- [ ] Add multilingual query support (translate + encode).
- [ ] Incorporate Reinforcement via Feedback (RLFH) for retrieval quality tuning.
- [ ] Connect to LlamaIndex or ColPali for structured multimodal reasoning.
- [ ] Build dataset versioning and evaluation pipeline for fine-tuning.

## ğŸ Final Vision

**"A unified multimodal system that understands, retrieves, and explains video content in human language â€” combining perception, retrieval, and reasoning."**
